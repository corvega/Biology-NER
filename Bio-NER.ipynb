{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bio - Named Entity Recognition\n",
    "Preprocess -> Extract basic word features -> POS tagging -> extract sentence level features -> Entity recognition\n",
    "\n",
    "The goal of NER is to provide a label for words or groups of words in a text sequence with an entity name or no entity. There is an assumption here that there are no overlapping entities. The bio_nlp2004 dataset contains tokenised text sequences with a matching list that contains a number representing an entity (0 is used to signify no entity). The numbers map to entity names. Each defined entity begins with ‘B-‘ or ‘I-‘ representing the beginning of a new entity and then a token that is inside an entity respectively (Jurafsky & Martin, 2021). The tokens are used as features and the tags are used as the labels for training the model. To improve the performance of NER we can add additional features such as Part-of-speech (POS) tagging. This assigns each token, of a document, a tag that describes its grammatical properties. By using this as an additional feature for NER we can maintain some information from the sentence structure and each token’s grammatical role in a sentence. It also allows for patters that relate to sentence structure to be identified. This should improve performance for entities that contain multiple works with distinct POS attributes, such as nouns. Bi-grams can also be used as an additional feature. These allows the context of tokens, based on their neighbours, to be preserved. This should improve entity recognition when an entity depends on neighbouring words, such as ‘in’ followed by an entity. For implementation the data is processed by adding POS tagging and previous words and next words are added (a similar idea to using bi grams). A conditional random field (CRF) model is instantiated. This uses neighbouring tags and features to calculate tag probabilities and identify named entities (Griffiths & Steyvers, 2004). This model is then trained, with the training data, containing the word ids as input tokens. The NLTK library is used for modelling and pre-processing; the pos_tag module for POS Tagging and the CRF tagger for the model. Punctuation, suffix and prev/next word processing is implemented in the code. For the f1 scores a function is used that perform the calculations for each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/C289216/.cache/huggingface/modules/datasets_modules/datasets/tner--bionlp2004/9f41d3f0270b773c2762dee333ae36c29331e2216114a57081f77639fdb5e904 (last modified on Wed Apr  5 15:55:51 2023) since it couldn't be found locally at tner/bionlp2004., or remotely on the Hugging Face Hub.\n",
      "Reusing dataset bio_nlp2004 (./data_cache/tner___bio_nlp2004/bionlp2004/1.0.0/9f41d3f0270b773c2762dee333ae36c29331e2216114a57081f77639fdb5e904)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d24ce31c0245d198c9a492c882ae53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is a dictionary with 3 splits: \n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 16619\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 1927\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 3856\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#Loading the data\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Use HuggingFace's datasets library\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# Loading the BIONLP 2004 dataset\n",
    "dataset = load_dataset(\n",
    "    \"tner/bionlp2004\", \n",
    "    cache_dir='./data_cache'\n",
    ")\n",
    "\n",
    "print(f'The dataset is a dictionary with {len(dataset)} splits: \\n\\n{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_ner = [item['tokens'] for item in dataset['train']]\n",
    "train_labels_ner = [[str(tag) for tag in item['tags']] for item in dataset['train']]\n",
    "\n",
    "val_sentences_ner = [item['tokens'] for item in dataset['validation']]\n",
    "val_labels_ner = [[str(tag) for tag in item['tags']] for item in dataset['validation']]\n",
    "\n",
    "test_sentences_ner = [item['tokens'] for item in dataset['test']]\n",
    "test_labels_ner = [[str(tag) for tag in item['tags']] for item in dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences = 16619\n",
      "Number of validation sentences = 1927\n",
      "Number of test sentences = 3856\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training sentences = {len(train_sentences_ner)}')\n",
    "print(f'Number of validation sentences = {len(val_sentences_ner)}')\n",
    "print(f'Number of test sentences = {len(test_sentences_ner)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does one instance look like from the training set? \n",
      "\n",
      "['Hence', ',', 'PPAR', 'can', 'positively', 'or', 'negatively', 'influence', 'TH', 'action', 'depending', 'on', 'TRE', 'structure', 'and', 'THR', 'isotype', '.']\n",
      "...and here is its corresponding label \n",
      "\n",
      "['0', '0', '3', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '3', '4', '0']\n"
     ]
    }
   ],
   "source": [
    "print(f'What does one instance look like from the training set? \\n\\n{train_sentences_ner[234]}')\n",
    "print(f'...and here is its corresponding label \\n\\n{train_labels_ner[234]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: ['0' '1' '10' '2' '3' '4' '5' '6' '7' '8' '9']\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of unique labels: {np.unique(np.concatenate(train_labels_ner))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'O', '1': 'B-DNA', '2': 'I-DNA', '3': 'B-protein', '4': 'I-protein', '5': 'B-cell_type', '6': 'I-cell_type', '7': 'B-cell_line', '8': 'I-cell_line', '9': 'B-RNA', '10': 'I-RNA'}\n"
     ]
    }
   ],
   "source": [
    "id2label = {\n",
    "    \"O\": \"0\",\n",
    "    \"B-DNA\": \"1\",\n",
    "    \"I-DNA\": \"2\",\n",
    "    \"B-protein\": \"3\",\n",
    "    \"I-protein\": \"4\",\n",
    "    \"B-cell_type\": \"5\",\n",
    "    \"I-cell_type\": \"6\",\n",
    "    \"B-cell_line\": \"7\",\n",
    "    \"I-cell_line\": \"8\",\n",
    "    \"B-RNA\": \"9\",\n",
    "    \"I-RNA\": \"10\"\n",
    "}\n",
    "\n",
    "label2id = {v:k for k, v in id2label.items()}\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Since',\n",
       " 'HUVECs',\n",
       " 'released',\n",
       " 'superoxide',\n",
       " 'anions',\n",
       " 'in',\n",
       " 'response',\n",
       " 'to',\n",
       " 'TNF',\n",
       " ',',\n",
       " 'and',\n",
       " 'H2O2',\n",
       " 'induces',\n",
       " 'VCAM-1',\n",
       " ',',\n",
       " 'PDTC',\n",
       " 'may',\n",
       " 'act',\n",
       " 'as',\n",
       " 'a',\n",
       " 'radical',\n",
       " 'scavenger',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences_ner[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = []\n",
    "# for each word in the sentence in the training set train_sentences_ner add a tuple of the word and its label from train_labels_ner\n",
    "for sentence, labels in zip(train_sentences_ner, train_labels_ner):\n",
    "    train_set.append([(word, label2id[label]) for word, label in zip(sentence, labels)])\n",
    "\n",
    "# map each label in the training set to its corresponding name in the label2id dictionary\n",
    "# train_set = [[(word, label2id[label]) for (word, label) in sentence] for sentence in train_set]\n",
    "\n",
    "\n",
    "test_set = []\n",
    "for sentence, labels in zip(test_sentences_ner, test_labels_ner):\n",
    "    test_set.append([(word, label2id[label]) for word, label in zip(sentence, labels)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'B-protein',\n",
       " 'I-protein',\n",
       " 'O',\n",
       " 'B-cell_type',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace the values of each array in test_labels_ner with the corresponding values in the label2id dictionary\n",
    "test_tags = [[label2id[label] for label in labels] for labels in test_labels_ner]\n",
    "test_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Number',\n",
       " 'of',\n",
       " 'glucocorticoid',\n",
       " 'receptors',\n",
       " 'in',\n",
       " 'lymphocytes',\n",
       " 'and',\n",
       " 'their',\n",
       " 'sensitivity',\n",
       " 'to',\n",
       " 'hormone',\n",
       " 'action',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens = test_sentences_ner\n",
    "test_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Train a CRF NER tagger\n",
    "def train_CRF_NER_tagger(train_set):\n",
    "    tagger = nltk.tag.CRFTagger()\n",
    "    tagger.train(train_set, 'model.crf.tagger')\n",
    "    return tagger  # return the trained model\n",
    "\n",
    "tagger = train_CRF_NER_tagger(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = tagger.tag_sents(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for class protein = 0.6529513555085561\n",
      "F1 score for class cell_type = 0.6308724832214766\n",
      "F1 score for class DNA = 0.5829145728643217\n",
      "F1 score for class cell_line = 0.4780793319415449\n",
      "F1 score for class RNA = 0.6017699115044248\n",
      "Macro-average f1 score = 0.5893175310080648\n"
     ]
    }
   ],
   "source": [
    "def extract_spans(tagged_sents):\n",
    "    spans = {}\n",
    "        \n",
    "    for sidx, sent in enumerate(tagged_sents):\n",
    "        start = -1\n",
    "        entity_type = None\n",
    "        for i, (tok, lab) in enumerate(sent):\n",
    "            if 'B-' in lab:\n",
    "                start = i\n",
    "                end = i + 1\n",
    "                entity_type = lab[2:]\n",
    "            elif 'I-' in lab:\n",
    "                end = i + 1\n",
    "            elif lab == 'O' and start >= 0:\n",
    "                \n",
    "                if entity_type not in spans:\n",
    "                    spans[entity_type] = []\n",
    "                \n",
    "                spans[entity_type].append((start, end, sidx))\n",
    "                start = -1      \n",
    "        # Sometimes an I-token is the last token in the sentence, so we still have to add the span to the list\n",
    "        if start >= 0:    \n",
    "            if entity_type not in spans:\n",
    "                spans[entity_type] = []\n",
    "                \n",
    "            spans[entity_type].append((start, end, sidx))\n",
    "                \n",
    "    return spans\n",
    "\n",
    "\n",
    "def cal_span_level_f1(test_sents, test_sents_with_pred):\n",
    "    # get a list of spans from the test set labels\n",
    "    gold_spans = extract_spans(test_sents)\n",
    "\n",
    "    # get a list of spans predicted by our tagger\n",
    "    pred_spans = extract_spans(test_sents_with_pred)\n",
    "    \n",
    "    # compute the metrics for each class:\n",
    "    f1_per_class = []\n",
    "    \n",
    "    ne_types = gold_spans.keys()  # get the list of named entity types (not the tags)\n",
    "    \n",
    "    for ne_type in ne_types:\n",
    "        # compute the confusion matrix\n",
    "        true_pos = 0\n",
    "        false_pos = 0\n",
    "        \n",
    "        for span in pred_spans[ne_type]:\n",
    "            if span in gold_spans[ne_type]:\n",
    "                true_pos += 1\n",
    "            else:\n",
    "                false_pos += 1\n",
    "                \n",
    "        false_neg = 0\n",
    "        for span in gold_spans[ne_type]:\n",
    "            if span not in pred_spans[ne_type]:\n",
    "                false_neg += 1\n",
    "                \n",
    "        if true_pos + false_pos == 0:\n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = true_pos / float(true_pos + false_pos)\n",
    "            \n",
    "        if true_pos + false_neg == 0:\n",
    "            recall = 0\n",
    "        else:\n",
    "            recall = true_pos / float(true_pos + false_neg)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "            \n",
    "        f1_per_class.append(f1)\n",
    "        print(f'F1 score for class {ne_type} = {f1}')\n",
    "        \n",
    "    print(f'Macro-average f1 score = {np.mean(f1_per_class)}')\n",
    "\n",
    "cal_span_level_f1(test_set, predicted_tags)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including NER Features:  POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "class CustomCRFTagger(nltk.tag.CRFTagger):\n",
    "    _current_tokens = None\n",
    "\n",
    "    def _get_features(self, tokens, idx):\n",
    "        \"\"\"\n",
    "        The function extracts features for a token.\n",
    "        \"\"\"\n",
    "        token = tokens[idx]\n",
    "        feature_list = []\n",
    "\n",
    "        if not token:\n",
    "            return feature_list\n",
    "\n",
    "        # Punctuation\n",
    "        punc_cat = set([\"Pc\", \"Pd\", \"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"])\n",
    "        if all(unicodedata.category(x) in punc_cat for x in token):\n",
    "            feature_list.append(\"PUNCTUATION\")\n",
    "\n",
    "        # Suffix up to length 3\n",
    "        if len(token) > 1:\n",
    "            feature_list.append(\"SUF_\" + token[-1:])\n",
    "        if len(token) > 2:\n",
    "            feature_list.append(\"SUF_\" + token[-2:])\n",
    "        if len(token) > 3:\n",
    "            feature_list.append(\"SUF_\" + token[-3:])\n",
    "\n",
    "        # Current word\n",
    "        feature_list.append(\"WORD_\" + token)\n",
    "\n",
    "        # Previous word\n",
    "        if idx > 0:\n",
    "            feature_list.append(\"PREVWORD_\" + tokens[idx-1])\n",
    "        # Next word\n",
    "        if idx < len(tokens)-1:\n",
    "            feature_list.append(\"NEXTWORD_\" + tokens[idx+1])\n",
    "\n",
    "        return feature_list\n",
    "\n",
    "\n",
    "class CRFTaggerWithPOS(CustomCRFTagger):\n",
    "    _current_tokens = None\n",
    "\n",
    "    def _get_features(self, tokens, index):\n",
    "        \"\"\"\n",
    "        Extract the features for a token and append the POS tag as an additional feature.\n",
    "        \"\"\"\n",
    "        basic_features = super()._get_features(tokens, index)\n",
    "\n",
    "        # Get the pos tags for the current sentence and save it\n",
    "        if tokens != self._current_tokens:\n",
    "            self._pos_tagged_tokens = nltk.pos_tag(tokens)\n",
    "            self._current_tokens = tokens\n",
    "\n",
    "        # Add POS tag to the features\n",
    "        basic_features.append(self._pos_tagged_tokens[index][1])\n",
    "\n",
    "        return basic_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for class protein = 0.6850019033117625\n",
      "F1 score for class cell_type = 0.7015918958031838\n",
      "F1 score for class DNA = 0.6393606393606395\n",
      "F1 score for class cell_line = 0.5413687436159346\n",
      "F1 score for class RNA = 0.6206896551724138\n",
      "Macro-average f1 score = 0.6376025674527868\n",
      "F1 Score:  None\n"
     ]
    }
   ],
   "source": [
    "def train_CRF_NER_tagger_with_POS(train_set):\n",
    "    tagger = CRFTaggerWithPOS()\n",
    "    tagger.train(train_set, 'model.crf.tagger')\n",
    "    return tagger\n",
    "\n",
    "\n",
    "# Train the model\n",
    "tagger = train_CRF_NER_tagger_with_POS(train_set)\n",
    "\n",
    "# Generate predictions\n",
    "predicted_tags = tagger.tag_sents([[t[0] for t in sentence] for sentence in test_set])\n",
    "\n",
    "# Now score the model\n",
    "score = cal_span_level_f1(test_set, predicted_tags)\n",
    "print(\"F1 Score: \", score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "We can see that the pre-processing steps offered improvement for every entity type. This shows that the grammatical information the POS tagging includes is helpful in entity identification as hypothesised. These results show that in both models ‘cell_line’ entity recognition performed the worst. This approach to NER has offered somewhat positive results. One area where this model would perform poorly is when entities that did not appear in the training data are used. To tackle this a hand-crafted dictionary could be used in training, that contains all the possible protein/ cell_type/ DNA/ cell_line/ RNA entities. This would make the model much more capable of handling entities that are out of the vocabulary of the training set. Another approach to tackle this problem would be to utilise a pretrained large language model such as BERT. These models have been trained of vast corpuses of text and can better handle complex patterns in text. Overall NER is a good approach to this task as biological names are comparatively distinct, which makes them easy to identify. This solution could be applied to identify similar published texts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "data_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
